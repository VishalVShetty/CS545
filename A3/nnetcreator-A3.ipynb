{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fcad260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting neuralnetworks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neuralnetworks.py\n",
    "import numpy as np\n",
    "import optimizers as opt\n",
    "import sys  # for sys.float_info.epsilon\n",
    "import qdalda\n",
    "\n",
    "######################################################################\n",
    "## class NeuralNetwork()\n",
    "######################################################################\n",
    "##ParentClass\n",
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden_units_by_layers, n_outputs):\n",
    "        '''\n",
    "        n_inputs: int\n",
    "        n_hidden_units_by_layers: list of ints, or empty\n",
    "        n_outputs: int\n",
    "        '''\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden_units_by_layers = n_hidden_units_by_layers\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        # Build list of shapes for weight matrices in each layera\n",
    "        shapes = []\n",
    "        n_in = n_inputs\n",
    "        for nu in self.n_hidden_units_by_layers + [n_outputs]:\n",
    "            shapes.append((n_in + 1, nu))\n",
    "            n_in = nu\n",
    "\n",
    "        self.all_weights, self.Ws = self._make_weights_and_views(shapes)\n",
    "        self.all_gradients, self.Grads = self._make_weights_and_views(shapes)\n",
    "\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.X_means = None\n",
    "        self.X_stds = None\n",
    "        self.T_means = None\n",
    "        self.T_stds = None\n",
    "\n",
    "    def _make_weights_and_views(self, shapes):\n",
    "        '''\n",
    "        shapes: list of pairs of ints for number of rows and columns\n",
    "                in each layer\n",
    "        Returns vector of all weights, and views into this vector\n",
    "                for each layer\n",
    "        '''\n",
    "        all_weights = np.hstack([np.random.uniform(size=shape).flat\n",
    "                                 / np.sqrt(shape[0])\n",
    "                                 for shape in shapes])\n",
    "        # Build list of views by reshaping corresponding elements\n",
    "        # from vector of all weights into correct shape for each layer.\n",
    "        views = []\n",
    "        first_element = 0\n",
    "        for shape in shapes:\n",
    "            n_elements = shape[0] * shape[1]\n",
    "            last_element = first_element + n_elements\n",
    "            views.append(all_weights[first_element:last_element]\n",
    "                         .reshape(shape))\n",
    "            first_element = last_element\n",
    "\n",
    "        return all_weights, views\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NeuralNetwork({self.n_inputs}, ' + \\\n",
    "            f'{self.n_hidden_units_by_layers}, {self.n_outputs})'\n",
    "\n",
    "    def train(self, X, T, n_epochs, method='sgd', learning_rate=None, verbose=True):\n",
    "        '''\n",
    "        X: n_samples x n_inputs matrix of input samples, one per row\n",
    "        T: n_samples x n_outputs matrix of target output values,\n",
    "            one sample per row\n",
    "        n_epochs: number of passes to take through all samples\n",
    "            updating weights each pass\n",
    "        method: 'sgd', 'adam', or 'scg'\n",
    "        learning_rate: factor controlling the step size of each update\n",
    "        '''\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        # Setup standardization parameters\n",
    "        if self.X_means is None:\n",
    "            self.X_means = X.mean(axis=0)\n",
    "            self.X_stds = X.std(axis=0)\n",
    "            self.X_stds[self.X_stds == 0] = 1\n",
    "            self.T_means = T.mean(axis=0)\n",
    "            self.T_stds = T.std(axis=0)\n",
    "\n",
    "        # Standardize X and T\n",
    "        X = (X - self.X_means) / self.X_stds\n",
    "        T = (T - self.T_means) / self.T_stds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = opt.Optimizers(self.all_weights)\n",
    "\n",
    "        _error_convert_f = lambda err: (np.sqrt(err) * self.T_stds)[0]\n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self._error_f, self._gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        error_convert_f=_error_convert_f,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self._error_f, self._gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         error_convert_f=_error_convert_f,\n",
    "                                         verbose=verbose)\n",
    "\n",
    "        elif method == 'scg':\n",
    "\n",
    "            error_trace = optimizer.scg(self._error_f, self._gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        error_convert_f=_error_convert_f,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd', 'adam', or 'scg'\")\n",
    "\n",
    "        self.total_epochs += len(error_trace)\n",
    "        self.error_trace += error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods\n",
    "        # after training, such as:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        '''\n",
    "        X assumed to be standardized and with first column of 1's\n",
    "        '''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:  # forward through all but last layer\n",
    "            #print('at forward',self.Ys[-1].shape, W[1:, :].shape)\n",
    "            self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        return self.Ys\n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def _error_f(self, X, T):\n",
    "        Ys = self._forward(X)\n",
    "        mean_sq_error = np.mean((T - Ys[-1]) ** 2)\n",
    "        return mean_sq_error\n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def _gradient_f(self, X, T):\n",
    "        # Assumes forward_pass just called with layer outputs saved in self.Ys.\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "\n",
    "        # D is delta matrix to be back propagated\n",
    "        D = -(T - self.Ys[-1]) / (n_samples * n_outputs)\n",
    "        self._backpropagate(D)\n",
    "\n",
    "        return self.all_gradients\n",
    "\n",
    "    def _backpropagate(self, D):\n",
    "        # Step backwards through the layers to back-propagate the error (D)\n",
    "        n_layers = len(self.n_hidden_units_by_layers) + 1\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    "            self.Grads[layeri][1:, :] = self.Ys[layeri].T @ D\n",
    "            # gradient of just the bias weights\n",
    "            self.Grads[layeri][0:1, :] = np.sum(D, axis=0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if layeri > 0:\n",
    "                D = D @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "\n",
    "    def use(self, X):\n",
    "        '''X assumed to not be standardized'''\n",
    "        # Standardize X\n",
    "        X = (X - self.X_means) / self.X_stds\n",
    "        Ys = self._forward(X)\n",
    "        # Unstandardize output Y before returning it\n",
    "        return Ys[-1] * self.T_stds + self.T_means\n",
    "\n",
    "    def get_error_trace(self):\n",
    "        return self.error_trace\n",
    "\n",
    "\n",
    "    \n",
    "## Parent Class ends here Child Class Starts here ##\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "##ChildClass\n",
    "class NeuralNetworkClassifier(NeuralNetwork):\n",
    "    def __init__(self,n_inputs, n_hidden_units_by_layers, n_classes):\n",
    "        super().__init__(n_inputs, n_hidden_units_by_layers, n_classes)\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = self.__repr__()\n",
    "        if self.total_epochs > 0:\n",
    "            s += f'\\n Trained for {self.total_epochs} epochs.'\n",
    "            s += f'\\n Final standardized training error {self.error_trace[-1]:.4g}.'\n",
    "        return s\n",
    "\n",
    "    ##modified trina fucntion for classfier\n",
    "    def train(self, X, T, n_epochs, method='sgd', learning_rate=None, verbose=True):\n",
    "        '''\n",
    "        X: n_samples x n_inputs matrix of input samples, one per row\n",
    "        T: n_samples x n_outputs matrix of target output values,\n",
    "            one sample per row\n",
    "        n_epochs: number of passes to take through all samples\n",
    "            updating weights each pass\n",
    "        method: 'sgd', 'adam', or 'scg'\n",
    "        learning_rate: factor controlling the step size of each update\n",
    "        '''\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        # Setup standardization parameters\n",
    "        if self.X_means is None:\n",
    "            #self.X_means = X.mean(axis=0)\n",
    "            #self.X_stds = X.std(axis=0)\n",
    "            #self.X_stds[self.X_stds == 0] = 1\n",
    "            #written a function for this no need for thses lines of code\n",
    "            #self.T_means = T.mean(axis=0)\n",
    "            #self.T_stds = T.std(axis=0)\n",
    "            X = self.standardize(X)\n",
    "\n",
    "        # Standardize X and T\n",
    "        #X = (X - self.X_means) / self.X_stds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = opt.Optimizers(self.all_weights)\n",
    "        TI = self.makeIndicatorVars(T)\n",
    "\n",
    "        _error_convert_f = lambda nll: np.exp(-nll)\n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.neg_log_likelihood, self._gradient_f,\n",
    "                                        fargs=[X, TI], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        error_convert_f=_error_convert_f,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self.neg_log_likelihood, self._gradient_f,\n",
    "                                         fargs=[X, TI], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         error_convert_f=_error_convert_f,\n",
    "                                         verbose=verbose)\n",
    "\n",
    "        elif method == 'scg':\n",
    "\n",
    "            error_trace = optimizer.scg(self.neg_log_likelihood, self._gradient_f,\n",
    "                                        fargs=[X, TI], n_epochs=n_epochs,\n",
    "                                        error_convert_f=_error_convert_f,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd', 'adam', or 'scg'\")\n",
    "\n",
    "        self.total_epochs += len(error_trace)\n",
    "        self.error_trace += error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods\n",
    "        # after training, such as:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X assumed to be standardized and with first column of 1's\n",
    "        '''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:  # forward through all but last layer\n",
    "            #print('at forward',self.Ys[-1].shape, W[1:, :].shape)\n",
    "            self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self._softmax(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])) ## softmax called here to avoid calling it in Other functions that follow forward\n",
    "        return self.Ys\n",
    "\n",
    "    def _softmax(self, Y):\n",
    "        '''Apply to final layer weighted sum outputs'''\n",
    "        # Trick to avoid overflow\n",
    "        #print(Y)\n",
    "        #maxY = Y.max()#changed since list operation was not possible NOT WORKINg\n",
    "        Z = np.max(Y)\n",
    "        maxY = max(0,Z)\n",
    "        expY = np.exp(Y - maxY)\n",
    "        denom = expY.sum(1).reshape((-1, 1))\n",
    "        Y = expY / (denom + sys.float_info.epsilon)\n",
    "        return Y\n",
    "    \n",
    "    def neg_log_likelihood(self,X,TI):\n",
    "        # w = warg.reshape((-1,K))\n",
    "        Y = self.forward(X) #forward function called here\n",
    "        #print('Output from forward',Y)\n",
    "        #G = self._softmax(Y[-1]) ## softmax called within forward now for ease\n",
    "        #print('Output from softmax',Y[-1])\n",
    "        #print('Output from negLL',-np.mean(TI * np.log(Y[-1] + sys.float_info.epsilon)))\n",
    "        return -np.mean(TI * np.log(Y[-1] + sys.float_info.epsilon))\n",
    "\n",
    "    def standardize(self,X):\n",
    "        X_means = X.mean(axis=0)\n",
    "        X_stds = X.std(axis=0)\n",
    "        X_stds[X_stds == 0] = 1\n",
    "        return (X - X_means) / X_stds\n",
    "\n",
    "    def makeIndicatorVars(self,T):\n",
    "    # Make sure T is two-dimensional. Should be nSamples x 1.\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1))    \n",
    "        self.Tunique = np.unique(T) # needed for extracting unique values from T i.e the different class types\n",
    "        return (T == np.unique(T)).astype(int)\n",
    "   \n",
    "    def use(self, X,return_hidden_layer_outputs=False):\n",
    "        '''X assumed to not be standardized'''\n",
    "        # Standardize X\n",
    "        X = self.standardize(X)\n",
    "        Ys = self.forward(X)\n",
    "        classes= np.argmax(Ys[-1],axis=1).reshape(-1,1)\n",
    "        \n",
    "        classes_T = classes\n",
    "        \n",
    "        i = 0\n",
    "        while i <(classes.shape[0]):\n",
    "            for value in classes[i]:\n",
    "                classes_T[i] = self.Tunique[value]\n",
    "            i += 1 \n",
    "        #classes_T = [self.Tunique[value] for i in range(classes.shape[0]) for value in classes[i]] \n",
    "        \n",
    "        # Unstandardize output Y before returning it\n",
    "        #return Ys[-1] * self.T_stds + self.T_means\n",
    "        Zs = Ys[:-1]\n",
    "        return (classes_T,Ys[-1],Zs) if return_hidden_layer_outputs else (classes_T, Ys[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62413b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
